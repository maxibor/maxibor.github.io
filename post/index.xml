<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts | Maxime Borry</title><link>https://maximeborry.com/post/</link><atom:link href="https://maximeborry.com/post/index.xml" rel="self" type="application/rss+xml"/><description>Posts</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Maxime Borry</copyright><lastBuildDate>Wed, 10 Feb 2021 10:40:21 +0100</lastBuildDate><image><url>https://maximeborry.com/media/icon_hu4c4b160219548583b0b35e67c5987186_165157_512x512_fill_lanczos_center_2.png</url><title>Posts</title><link>https://maximeborry.com/post/</link></image><item><title>noSQL: when it's too big for a dictionary</title><link>https://maximeborry.com/post/rocksdb/</link><pubDate>Wed, 10 Feb 2021 10:40:21 +0100</pubDate><guid>https://maximeborry.com/post/rocksdb/</guid><description>&lt;p>Recently, I&amp;rsquo;ve been working on a project where I need to store a key-value pairs for very large files.
In Python, the first idea would be to use a &lt;a href="https://docs.python.org/3/tutorial/datastructures.html#dictionaries" target="_blank" rel="noopener">dictionary&lt;/a>.&lt;br>
However, I had more than 800 millions key-value pairs to store, so a dictonary just wouldn&amp;rsquo;t cut it.&lt;br>
Enters the world of noSQL databases, and more specifically &lt;a href="https://rocksdb.org" target="_blank" rel="noopener">RocksDB&lt;/a>.&lt;br>
Described by its creators, originally at Facebook, as a &amp;ldquo;high performance persistant key-value store&amp;rdquo;, it behaves pretty much like a dictonary, but can scale very well.&lt;/p>
&lt;p>Written in C++, there is fortunately a python binding, in the name of &lt;a href="https://github.com/twmht/python-rocksdb" target="_blank" rel="noopener">python-rocksdb&lt;/a>, even available as a &lt;a href="https://anaconda.org/conda-forge/python-rocksdb" target="_blank" rel="noopener">conda package&lt;/a>.&lt;/p>
&lt;p>It&amp;rsquo;s relatively simple to use:&lt;/p>
&lt;pre>&lt;code class="language-python">import rocksdb
# Create or open database
db = rocksdb.DB(&amp;quot;test.db&amp;quot;, rocksdb.Options(create_if_missing=True))
key = &amp;quot;Hello&amp;quot;
value = &amp;quot;World&amp;quot;
# Store key-value pair
db.put(key.encode(), value.decode())
# Get value for a given key
print(db.get(key.encode()).decode())
# prints &amp;quot;World&amp;quot;
# Delete a key value-pair
db.delete(key.encode())
&lt;/code>&lt;/pre>
&lt;p>Because RocksDB stores all data as byte strings, one need to use &lt;code>.encode()&lt;/code> and &lt;code>.decode()&lt;/code> to store and get data.&lt;/p>
&lt;h3 id="batch-operation">Batch operation&lt;/h3>
&lt;p>One way to speed up RockDB, is to perform operations in batch, instead of performing them one by one.&lt;/p>
&lt;pre>&lt;code class="language-python">import rocksdb
db = rocksdb.DB(&amp;quot;test.db&amp;quot;, rocksdb.Options(create_if_missing=True))
batch = rocksdb.WriteBatch()
batch.put(b&amp;quot;key&amp;quot;, b&amp;quot;v1&amp;quot;)
batch.delete(b&amp;quot;key&amp;quot;)
batch.put(b&amp;quot;key&amp;quot;, b&amp;quot;v2&amp;quot;)
batch.put(b&amp;quot;key&amp;quot;, b&amp;quot;v3&amp;quot;)
db.write(batch)
&lt;/code>&lt;/pre>
&lt;h3 id="batch-limits-and-solutions">Batch limits and solutions&lt;/h3>
&lt;p>&lt;strong>But beware !&lt;/strong> For very large files, the batch can become too big for the memory of your computer, and you end up &lt;a href="https://www.enterprisestorageforum.com/storage-hardware/memory-swapping.html" target="_blank" rel="noopener">swapping&lt;/a> like crazy !&lt;/p>
&lt;figure id="figure-when-memory-overflows-to-swap">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://media1.tenor.com/images/09e8655db8859a953eb58c64a9fc7d7e/tenor.gif?itemid=11213683" alt="When memory overflows to swap" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
When memory overflows to swap
&lt;/figcaption>&lt;/figure>
&lt;p>To prevent this, we have to restrict ourselves to batches of reasonable sizes, meaning making more batches, but smaller.&lt;/p>
&lt;p>&lt;strong>But beware !&lt;/strong> RocksDB storage system relies on many indivual &lt;code>.sst&lt;/code> files, that RocksDB opens in parallel to make queries and store data. The larger the database, the more files are open. And this can lead to this kind of error:&lt;/p>
&lt;pre>&lt;code class="language-bash">IO error: While open a file for appending: xxxxxxx.sst: Too many open files
&lt;/code>&lt;/pre>
&lt;p>Indeed, there is maximum number of files that can be open at any give time by a single process.&lt;br>
For example, it&amp;rsquo;s &lt;a href="https://stackoverflow.com/questions/6624077/max-open-files-per-process" target="_blank" rel="noopener">256 by default on macOS&lt;/a>&lt;/p>
&lt;p>Luckily, to overcome this hurdle, RocksDB has the &lt;code>max_open_files&lt;/code> option.&lt;/p>
&lt;p>Putting it all together, this gives us the following script:&lt;/p>
&lt;pre>&lt;code class="language-python">import rocksdb
from subprocess import check_output
from tqdm import tqdm
def get_nb_lines(filename):
&amp;quot;&amp;quot;&amp;quot;A function for getting the number of lines in a file
Args:
filename(str): The path to a file
Returns:
int: Number of lines in file
&amp;quot;&amp;quot;&amp;quot;
cmd = f&amp;quot;wc -l {filename}&amp;quot;
return int(check_output(cmd, shell=True).split()[0])
OPTS = rocksdb.Options()
OPTS.create_if_missing = False
OPTS.max_open_files = 250
# Instantiating the database
db = rocksdb.DB(&amp;quot;mybigdata.db&amp;quot;, OPTS)
# The big file we want to store in RocksDB
key_value_file = &amp;quot;very_large_file.tsv&amp;quot;
# We get the number of key-value pairs in the file
nb_key_value_pairs = get_nb_lines(key_value_file)
# Starting our first batch
batch = rocksdb.WriteBatch()
# Key-value pair counter
i = 0
# Number of batches, the more, the less memory used
max_batches = 100
# Setting the batch size: how many key-value pairs go in each batch
batch_size = min(nlines-1, int(nlines/max_batches))
with open(key_value_file) as bigfile:
for line in tqdm(bigfile, total=nlines):
# Each line looks like: key [tab] value
linesplit = line.split()
key = linesplit[0]
value = linesplit[1]
batch.put(bytes(key, encoding='utf8'), bytes(value, encoding='utf8'))
# If we reached the batch size, store it in RocksDB, and create a new batch
if i % batch_size == 0:
db.write(batch)
batch = rocksdb.WriteBatch()
# Store the remaining key-value pairs
db.write(batch)
&lt;/code>&lt;/pre></description></item><item><title>Making Interactive maps in Python using GeoJSON and GitHub</title><link>https://maximeborry.com/post/interactive-map-howto/</link><pubDate>Tue, 12 Jan 2021 13:08:30 +0100</pubDate><guid>https://maximeborry.com/post/interactive-map-howto/</guid><description>&lt;p>Recently, I&amp;rsquo;ve been involved with the &lt;a href="https://github.com/spaam-community/ancientMetagenomeDir" target="_blank" rel="noopener">AncientMetagenomeDir&lt;/a> project. Briefly, with this collaborative community effort, we aimed to regroup in one single repository, all the metadata about every single published ancient DNA metagenomics article, and turn them into &lt;a href="https://en.wikipedia.org/wiki/FAIR_data" target="_blank" rel="noopener">FAIR&lt;/a> scientific data.&lt;/p>
&lt;p>We ended with large &lt;code>TSV&lt;/code> (table) files regrouping a standardized set of metadata, about each ancient DNA metagenomics sample. Because these are originally archeological data, one of the information that is systematically collected is the geographical location of each sample.&lt;/p>
&lt;p>While static maps were already generated for the &lt;a href="https://doi.org/10.1101/2020.09.02.279570" target="_blank" rel="noopener">AncientMegenomeDir publication&lt;/a>, we had the opportunity to play with interactive maps for the &lt;a href="https://spaam-community.github.io/AncientMetagenomeDir/#/" target="_blank" rel="noopener">website of the project&lt;/a>.&lt;/p>
&lt;p>Usually, hosting interactive elements online require some sort of backend framework (like &lt;a href="https://www.streamlit.io/" target="_blank" rel="noopener">Streamlit&lt;/a> or &lt;a href="https://shiny.rstudio.com/" target="_blank" rel="noopener">Shiny&lt;/a>) to perform the rendering, however, I wanted to have it as serverless as possible, and this is where the &lt;a href="https://docs.github.com/en/free-pro-team@latest/github/managing-files-in-a-repository/mapping-geojson-files-on-github" target="_blank" rel="noopener">&lt;code>GeoJSON&lt;/code> rendering function of GitHub&lt;/a> came to the rescue.&lt;/p>
&lt;p>Using GitHub magic, that meant that as long as I would push a &lt;a href="https://geojson.org/" target="_blank" rel="noopener">&lt;code>GeoJSON&lt;/code>&lt;/a> file on GitHub, it would automatically be rendered as an interactive map, thanks to &lt;a href="https://leafletjs.com/" target="_blank" rel="noopener">Leaflet.js&lt;/a>.&lt;/p>
&lt;h2 id="from-tsv-to-geojson">From TSV to GeoJSON&lt;/h2>
&lt;p>The question that I was left with: How to go from a &lt;code>TSV&lt;/code> table to a &lt;code>GeoJSON&lt;/code> file ?
Luckily for me, this is really easy to do thanks to &lt;a href="https://geopandas.org/" target="_blank" rel="noopener">GeoPandas&lt;/a>.&lt;br>
I only needed to make sure that there were a &lt;code>latitude&lt;/code> and &lt;code>longitude&lt;/code> columns in the &lt;code>TSV&lt;/code> files.&lt;/p>
&lt;figure id="figure-geographic-coordinate-system-credit-wikipediahttpsenwikipediaorgwikigeographic_coordinate_system">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="Geographic coordinate system, credit: [Wikipedia](https://en.wikipedia.org/wiki/Geographic_coordinate_system)" srcset="
/post/interactive-map-howto/coordinate-system_hu0fbc7a9d6e3ec1414817c2d1317b72fa_45308_e68793dd344fa7bdbd49fc64a155d63d.png 400w,
/post/interactive-map-howto/coordinate-system_hu0fbc7a9d6e3ec1414817c2d1317b72fa_45308_253cc9b1a123bc88e7104702cf408d7b.png 760w,
/post/interactive-map-howto/coordinate-system_hu0fbc7a9d6e3ec1414817c2d1317b72fa_45308_1200x1200_fit_lanczos_2.png 1200w"
src="https://maximeborry.com/post/interactive-map-howto/coordinate-system_hu0fbc7a9d6e3ec1414817c2d1317b72fa_45308_e68793dd344fa7bdbd49fc64a155d63d.png"
width="435"
height="340"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Geographic coordinate system, credit: &lt;a href="https://en.wikipedia.org/wiki/Geographic_coordinate_system">Wikipedia&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;pre>&lt;code class="language-python">import pandas as pd
import geopandas
df = pd.read_csv(&amp;quot;table.tsv&amp;quot;, sep=&amp;quot;\t&amp;quot;)
gdf = geopandas.GeoDataFrame(df, geometry=geopandas.points_from_xy(df.longitude, df.latitude))
gdf.to_file(&amp;quot;output.geo.json&amp;quot;, driver='GeoJSON')
&lt;/code>&lt;/pre>
&lt;blockquote>
&lt;p>Instead of pushing to GitHub at every change to check a GeoJSON rendering, you can check a GeoJSON map with the &lt;a href="https://geojson.io/" target="_blank" rel="noopener">geojson.io&lt;/a> website.&lt;/p>
&lt;/blockquote>
&lt;h2 id="displaying-more-metadata-on-the-map">Displaying more metadata on the map&lt;/h2>
&lt;p>So far, I only used the map to display the latitude and longitude of each sample, but we can actually display more information by changing the color, size, or the shape of each marker(point) for example.&lt;/p>
&lt;p>Refering again the &lt;a href="https://docs.github.com/en/free-pro-team@latest/github/managing-files-in-a-repository/mapping-geojson-files-on-github#styling-features" target="_blank" rel="noopener">Github documentation&lt;/a>, this corresponds to the &lt;code>marker-color&lt;/code>, &lt;code>marker-size&lt;/code>, or &lt;code>marker-symbol&lt;/code>.&lt;/p>
&lt;p>For example, to change the color, we add a &lt;code>marker-color&lt;/code> column with the desired color value.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;strong>marker-color&lt;/strong>&lt;/th>
&lt;th>publication_doi&lt;/th>
&lt;th>site_name&lt;/th>
&lt;th>latitude&lt;/th>
&lt;th>longitude&lt;/th>
&lt;th>sample_name&lt;/th>
&lt;th>sample_age&lt;/th>
&lt;th>material&lt;/th>
&lt;th>archive&lt;/th>
&lt;th>archive_accession&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>#009C54&lt;/td>
&lt;td>10.1016/j.quascirev.2017.11.037&lt;/td>
&lt;td>Hässeldala Port&lt;/td>
&lt;td>56.16&lt;/td>
&lt;td>15.01&lt;/td>
&lt;td>HA1.1&lt;/td>
&lt;td>13900&lt;/td>
&lt;td>lake sediment&lt;/td>
&lt;td>ENA&lt;/td>
&lt;td>SRS2040659&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>#C22026&lt;/td>
&lt;td>10.3390/geosciences10070270&lt;/td>
&lt;td>Unknown&lt;/td>
&lt;td>53.322&lt;/td>
&lt;td>1.118&lt;/td>
&lt;td>ELF001A_95_S81_ELFM1D1&lt;/td>
&lt;td>6000&lt;/td>
&lt;td>shallow marine sediment&lt;/td>
&lt;td>ENA&lt;/td>
&lt;td>ERS3605424&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Table 1:&lt;/strong> Sample data from the AncientMetagenomeDir repository&lt;/p>
&lt;figure id="figure-markers-are-colored-by-property">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="Markers are colored by property" srcset="
/post/interactive-map-howto/marker-color_hu5de8302c11b0599c4f8a330d955cd7b3_216765_2dc745ec159793e84743ce05c605094e.png 400w,
/post/interactive-map-howto/marker-color_hu5de8302c11b0599c4f8a330d955cd7b3_216765_cf4de92bc17b1b08278892512e81d135.png 760w,
/post/interactive-map-howto/marker-color_hu5de8302c11b0599c4f8a330d955cd7b3_216765_1200x1200_fit_lanczos_2.png 1200w"
src="https://maximeborry.com/post/interactive-map-howto/marker-color_hu5de8302c11b0599c4f8a330d955cd7b3_216765_2dc745ec159793e84743ce05c605094e.png"
width="632"
height="415"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Markers are colored by property
&lt;/figcaption>&lt;/figure>
&lt;p>Here, markers in pink are host-associated single genomes, while markers in light-blue are host-associated metagenomes.&lt;/p>
&lt;h2 id="preventing-overlapping-points">Preventing overlapping points&lt;/h2>
&lt;p>In this dataset, different samples are sometimes coming from a same archeological site. In practice, this means that points will overlap on the map because they share the exact same geographic coordinates.
In Figure 2, for example, you can notice a very dark shadow bellow each marker: that&amp;rsquo;s because there are many overlapping markers present on the spot.&lt;/p>
&lt;p>The problem is that only one marker will be displayed, and the other ones being hidden below.&lt;/p>
&lt;p>To overcome this issue, the little trick is to slightly alter the coordinates of each sample to plot them as distinct points on the map. I did that with random sampling from the normal distribution using &lt;a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html" target="_blank" rel="noopener">Numpy&lt;/a> with a very small standard deviation.&lt;/p>
&lt;pre>&lt;code class="language-python">import pandas as pd
import geopandas
import numpy as np
df = pd.read_csv(&amp;quot;table.tsv&amp;quot;, sep=&amp;quot;\t&amp;quot;)
sigma = 0.0015
df['new_latitude'] = df['latitude'].apply(lambda x: np.random.normal(x, sigma))
df['new_longitude'] = df['longitude'].apply(lambda x: np.random.normal(x, sigma))
gdf = geopandas.GeoDataFrame(df, geometry=geopandas.points_from_xy(df.new_longitude, df.new_latitude))
gdf.to_file(&amp;quot;output.geo.json&amp;quot;, driver='GeoJSON')
&lt;/code>&lt;/pre>
&lt;p>Problem solved !&lt;/p>
&lt;figure id="figure-overalapping-markers-are-jittered-around-the-exact-coordinates">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >
&lt;img alt="Overalapping markers are &amp;#39;jittered&amp;#39; around the exact coordinates" srcset="
/post/interactive-map-howto/marker-jitter_hud42370f9c294ed1624b2767a29b5f442_213929_7e8795447b89cbb94ef9bbc4dd5c1c3e.png 400w,
/post/interactive-map-howto/marker-jitter_hud42370f9c294ed1624b2767a29b5f442_213929_0e366325b86badb3a5d1afabc89ed3de.png 760w,
/post/interactive-map-howto/marker-jitter_hud42370f9c294ed1624b2767a29b5f442_213929_1200x1200_fit_lanczos_2.png 1200w"
src="https://maximeborry.com/post/interactive-map-howto/marker-jitter_hud42370f9c294ed1624b2767a29b5f442_213929_7e8795447b89cbb94ef9bbc4dd5c1c3e.png"
width="635"
height="399"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Overalapping markers are &amp;lsquo;jittered&amp;rsquo; around the exact coordinates
&lt;/figcaption>&lt;/figure>
&lt;h2 id="end-result">End result&lt;/h2>
&lt;p>Finally, thanks to the magic of GitHub GeoJSON rendering, the map can be easily embedded on any web page !&lt;/p>
&lt;script src="https://embed.githubusercontent.com/view/geojson/SPAAM-community/AncientMetagenomeDir/master/assets/analysis/live/ancientmetagenomedir.geo.json?height=400&amp;width=800">&lt;/script>
&lt;p>This is an interactive map, you can click on a marker to look at the details.&lt;/p></description></item></channel></rss>
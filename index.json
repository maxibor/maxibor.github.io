[{"authors":["maxime"],"categories":null,"content":"I am Maxime, a Doctoral Researcher in Bioinformatics at the Max Planck Institute for the Science History, and the Max Planck Institute for Evolutionary Anthropology in Germany.\nAfter an undergraduate degree in biology, I first decided to dive in the studies of population and our environment. Only after realizing I wanted to focus more on the statistical part and the data analysis, I switched back to a Bioinformatics MSc.\nI am now working in the Microbiome Sciences research group on ancient DNA metagenomics, under the supervision of Christina Warinner.\n","date":1652182725,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1652182725,"objectID":"e039f055c184f6b656cae4531387a7fc","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am Maxime, a Doctoral Researcher in Bioinformatics at the Max Planck Institute for the Science History, and the Max Planck Institute for Evolutionary Anthropology in Germany.\nAfter an undergraduate degree in biology, I first decided to dive in the studies of population and our environment.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":[],"content":"I\u0026rsquo;ve been recently reminded of the Monty Hall problem, and I thought I would try to show how to solve it with simulations in Python.\nInspired from the 60\u0026rsquo;s American TV game show Let\u0026rsquo;s Make a Deal, the principle is the following: The player is faced with three doors, behind one of which is a big prize (a üöó), and behind the two others, a üêê, and has to choose one, without opening it yet.\nAfter the participant chose a door, Monty Hall (the host of the show) opens one of the two other door, behind which is always a üêê.\nWhat is the best strategy to win the üöó ?\nKeeping the original choice, or changing of door ?\nIt turns out that\u0026hellip;\nRead further to find out üòâ\nimport numpy as np import altair as alt import pandas as pd alt.renderers.enable('mimetype')  nb_games = 10000 # The number of games we're going to simulate  games = [] for i in range(nb_games): game = np.repeat('üêê', 3) prize = np.random.randint(0,3,1) # the door behind which the car is hidden game[prize]='üöó' games.append(game)  Strategy: keep door wins=0 # win counter for i in range(nb_games): doors = [0,1,2] player_door_choice = int(np.random.choice(doors, 1)) # the player initial's door choice doors.remove(player_door_choice) # doors not chosen by player for j in doors: # Monty Hall reveals that behind one of the door not chosen is a goat if games[i][j] == 'üêê': goat_door = j if games[i][player_door_choice] == 'üöó': # The player keeps his initial door choice wins += 1  print(f\u0026quot;Winning percentage: {round(wins/nb_games*100,1)}%\u0026quot;)  Winning percentage: 32.1%  x = ['üöó','üêê'] y = [round(wins/nb_games*100, 0), 100 - round(wins/nb_games*100, 0)] source = pd.DataFrame({'x':x, 'y':y}) (alt.Chart(source).mark_bar(size=30).encode( x=alt.X('x', axis=alt.Axis(title=None)), y=alt.Y('y', axis=alt.Axis(title='outcome %')) ).properties(width=200, title='Strategy: keep same door') .configure_axis(labelFontSize=20) .configure_title( fontSize=20, font='Courier', anchor='start', color='gray' ))  Strategy: change door wins=0 for i in range(nb_games): doors = [0,1,2] player_door_choice = int(np.random.choice(doors, 1)) # the player initial's door choice doors.remove(player_door_choice) # doors not chosen for j in doors: # Monty Hall reveals that behind one of the door not chosen is a goat if games[i][j] == 'üêê': goat_door = j doors.remove(goat_door) # the player changes its door choice if games[i][doors[0]] == 'üöó': wins += 1  print(f\u0026quot;Winning percentage: {round(wins/nb_games*100,1)}%\u0026quot;)  Winning percentage: 66.7%  x = ['üöó','üêê'] y = [int(wins/nb_games*100), 100 - int(wins/nb_games*100)] source = pd.DataFrame({'x':x, 'y':y}) (alt.Chart(source).mark_bar(size=30).encode( x=alt.X('x', axis=alt.Axis(title=None)), y=alt.Y('y', axis=alt.Axis(title='outcome %')) ).properties(width=200, title='Strategy: change door') .configure_axis(labelFontSize=20) .configure_title( fontSize=20, font='Courier', anchor='start', color='gray' ))  As I showed above, the door changing strategy is much better with a 66.6% change of winning, vs 33.3% if we keep the original door.\nIf you want to learn more about the Monty Hall problem, and how to prove using the Baye\u0026rsquo;s theorem, have a reat at the wikipedia page\nFinally, the code behind this post is here\nLast but not least, if you want to make it interactive with JavaScript P5, please have a look at this video from the Coding Train, which inspired this blog post üôÇ\n     ","date":1652476395,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652476395,"objectID":"6cd21a56a9a522777a04dfc3983c04a4","permalink":"https://maximeborry.com/post/monty-hall-python/","publishdate":"2022-05-13T23:13:15+02:00","relpermalink":"/post/monty-hall-python/","section":"post","summary":"What is the best strategy to win a üöó ? Keeping the original choice, or changing of door ?","tags":[],"title":"Using Python to solve the Monty Hall problem","type":"post"},{"authors":[""],"categories":[],"content":"Introduction The task of taxonomic profiling consists of answering the question \u0026ldquo;Who is there ?\u0026quot; in a metagenomic sample. However, in practice, it turns out to be quite a challenging assignment to tackle, and many solutions have been proposed to address it (for a comparative benchmark, see Meyer et al. 2022).\nOne of the central aspect of taxonomic profiling is to distinguishing true positive (taxon actually present in a sample) from false positive (taxon not actually present in sample) taxonomic assignments.\nToday, I want to focus on one particular taxonomic profiler: KrakenUniq (Breitwieser et al. 2018).\nBased on the popular k-mer based taxonomic classifier Kraken (Wood et al. 2014), KrakenUniq adds a unique (üòâ) feature on top of Kraken, the reporting of unique k-mer counts.\nLet me explain:\nThe original Kraken program is based on direct k-mer matching. First Kraken builds a database of all present k-mer in each reference genomes, and then compares it to the k-mers found in the query sequences/sequencing reads (fig 1) . If there is a match, it uses a taxonomic tree and the LCA algorithm to assign each query sequence to a given taxon.\nFigure 1: The Kraken sequence classification algorithm. original here\nWhile this is already a good performing taxonomic profiling method, it suffers from one major drawback: it can not account for duplicated sequences. For example, when reporting the number of reads belonging to each taxon in a sample, Kraken is not able to distinguish an evenly covered genome (likely a true positive, blue genome fig 2) from a genome suffering from read stacking (likely a false positive, purple genome, fig 2). This uneven coverage, often a false positive assignment, can be the consequence of different reasons, but one of the most common is due to reads matching ultra-conserved regions between taxons from the same clade, the so-called read stacking.\nFigure 2: Two taxons having the same amount of assigned reads, two different scenarios. The blue genome has an evenly distributed coverage, while the purple genome suffers from read stacking. Unique kmers are highlighted with a black perimeter\nTo circumvent this problem, KrakenUniq leverages the HLL algorithm to count the unique/distinct k-mers. In practice, in the example of fig 2, Kraken would have reported a k-mer count of 24 for both the blue and purple genome, while KrakenUniq would have reported a k-mer count of 7 for the blue genome, and k-mer count of 2 for the purple genome.\nOn top of that, KrakenUniq keeps track of how many of unique k-mers have been found, out all possible unique k-mers for each taxon, and normalizes it by the taxon genome size, which gives an estimation of the \u0026ldquo;coverage\u0026rdquo;.\nSo, to summarize, with KrakenUniq, for each taxon, we now have three different metrics instead of \u0026ldquo;just one\u0026rdquo; for Kraken :\n read count per taxon (which we already had with kraken), we\u0026rsquo;ll refer to it later as $R$ number of unique k-mers (new with KrakenUniq), we\u0026rsquo;ll refer to it later as $K$ \u0026ldquo;coverage\u0026rdquo; of the k-mers of the clade in the database, we\u0026rsquo;ll refer to it later as $C$  Note that I\u0026rsquo;ve always mentioned the \u0026ldquo;coverage\u0026rdquo; between quotes, this is because KrakenUniq defines it as $C = \\frac{K}{genome\\ size}$.\nBecause by definition, the maximum number of k-mers (of length $k$) for a sequence of length $L$ can not exceed $L - k + 1$. This means that in the ideal situation, for example the blue genome of fig 2, the \u0026ldquo;coverage\u0026rdquo; will be at best, close to 1, but never greater than 1.\nTo try to make sense of these three metrics, Guellil et al. 2022 came up with a score $E$ combining them, to discriminate true positive from false positive taxonomic assignments.\n$$E = \\frac{K}{R} \\times C$$\nHere, using different simulated taxon assignment situations, I propose a tweak to this score, to try to improve its ability to differentiate true positive from false positive taxonomic assignments.\nThe simulations First, let\u0026rsquo;s define a few parameters for our simulation, and import necessary libraries\nimport numpy as np import matplotlib.pyplot as plt import seaborn as sns import pandas as pd read_length = 45 kmer_length = 35 genome_size = 1000000  We\u0026rsquo;ll look at four different possible scenarios\nFor each scenario, we\u0026rsquo;ll simulate 100 different situations\nscenario A: high duplication, low coverage (false positive)  A lot of reads are duplicated, probably coming from conversed regions, or sequencing artifacts low coverage There are more reads than unique k-mers  a_reads = np.random.randint(10, 300, 100) # randomly choose the number of reads, between 10 and 300 comp_kmer_a = lambda x: int(x / np.random.randint(2,10, 1)) # randomly choose between 2 and 10 times less k-mers comp_kmer_a_vec = np.vectorize(comp_kmer_a) a_kmers = comp_kmer_a_vec(a_reads) plt.plot(a_kmers/a_reads, label=r\u0026quot;$\\frac{k}{R}$\u0026quot;) plt.plot(a_kmers/genome_size, label=\u0026quot;cov\u0026quot;) plt.title(\u0026quot;scenario A\u0026quot;) plt.legend(loc=\u0026quot;upper left\u0026quot;);  scenario B: low duplication, low coverage (true positive)  Very few reads are duplicated low coverage more unique k-mers than reads (up to $ku = read_length - kmer_length + 1$ more unique k-mers than reads)  b_reads = np.random.randint(10, 300, 100) # randomly choose the number of reads, between 10 and 300 comp_kmer_b = lambda x: min(genome_size - kmer_length + 1, int(x * np.random.randint(2,(read_length - kmer_length + 1), 1))) # randomly choose up to ku more k-mers than reads comp_kmer_b_vec = np.vectorize(comp_kmer_b) b_kmers = comp_kmer_b_vec(b_reads) plt.plot(b_kmers/b_reads, label=r\u0026quot;$\\frac{k}{R}$\u0026quot;) plt.plot(b_kmers/genome_size, label=\u0026quot;cov\u0026quot;) plt.title(\u0026quot;scenario B\u0026quot;) plt.legend(loc=\u0026quot;upper left\u0026quot;);  scenario C: low duplication, higher coverage (true positive)  same as scenario B, but with a higher coverage  c_reads = np.random.randint(10000, 100000, 100) # # randomly choose the number of reads, between 10000 and 100000 comp_kmer_c = lambda x: min(genome_size - kmer_length + 1, int(x * np.random.randint(2,(read_length - kmer_length + 1), 1))) comp_kmer_c_vec = np.vectorize(comp_kmer_c) c_kmers = comp_kmer_c_vec(c_reads) plt.plot(c_kmers/c_reads, label=r\u0026quot;$\\frac{k}{R}$\u0026quot;) plt.plot(c_kmers/genome_size, label=\u0026quot;cov\u0026quot;) plt.title(\u0026quot;scenario C\u0026quot;) plt.legend(loc=\u0026quot;upper left\u0026quot;);  scenario D: low duplication, high coverage  more reads than unique kmers (because of the coverage definiton of KrakenUniq) high coverage  d_reads = np.random.randint(genome_size*1.1, genome_size*3, 100) # getting between 1.1*genome_size and 3*genome size reads comp_kmer_d = lambda x: min(genome_size - kmer_length + 1, int(x * np.random.randint(2,(read_length - kmer_length + 1), 1))) comp_kmer_d_vec = np.vectorize(comp_kmer_d) d_kmers = comp_kmer_d_vec(d_reads) plt.plot(d_kmers/d_reads, label=r\u0026quot;$\\frac{k}{R}$\u0026quot;) plt.plot(d_kmers/genome_size, label=\u0026quot;cov\u0026quot;) plt.title(\u0026quot;scenario D\u0026quot;) plt.legend(loc=\u0026quot;upper left\u0026quot;);  Comparing E-scores Guellil et al. E-score Now that we have our four different scenarios, let\u0026rsquo;s look at how Guellil et al. E-score performs at discriminating between true positive (scenario A) and false positive (scenario B, C, and D) taxonomic assignments.\ndef e_score_guellil(nb_kmer, nb_read, genome_size): cov = nb_kmer/genome_size return((nb_kmer/nb_read) * cov ) e_score_guellil_vec = np.vectorize(e_score_guellil)  e_score_guellil_a = e_score_guellil_vec(a_kmers, a_reads, genome_size) e_score_guellil_b = e_score_guellil_vec(b_kmers, b_reads, genome_size) e_score_guellil_c = e_score_guellil_vec(c_kmers, c_reads, genome_size) e_score_guellil_d = e_score_guellil_vec(d_kmers, d_reads, genome_size) plt.plot(e_score_guellil_a, label = \u0026quot;scenario A\u0026quot;) plt.plot(e_score_guellil_b, label = \u0026quot;scenario B\u0026quot;) plt.plot(e_score_guellil_c, label = \u0026quot;scenario C\u0026quot;) plt.plot(e_score_guellil_d, label = \u0026quot;scenario D\u0026quot;) plt.legend(loc=\u0026quot;upper left\u0026quot;) plt.title(\u0026quot;Guellil et al. E score\u0026quot;) plt.ylim(0,0.001);  As we can see, it can be quite tricky in some situations to differentiate a true from a false positive assignment. This is particularly problematic between scenario A and scenario B.\nModified dexp E-score Here I propose a slightly different score to mitigate some of issue. Instead of defining\n$$E = \\frac{K}{R}\\times C$$\nI propose to use\n$$E = \\frac{K}{R} \\times dexp(C)$$\nwith $dexp$ being the double exponential function.\nThis way, more emphasis is given to the \u0026ldquo;coverage\u0026rdquo;, which particularly helps in low \u0026ldquo;coverage\u0026rdquo; scenarios, since the double exponential function grows \u0026ldquo;really fast\u0026rdquo; between 0 and 1.\n#https://en.wikipedia.org/wiki/Double_exponential_function def double_exp(x, a=1.3, b=18): return(a**(b*x)) double_exp_vec = np.vectorize(double_exp)  x = np.arange(0,1,0.01) y = double_exp_vec(x) plt.plot(x, y) plt.title(\u0026quot;Double exponential function\u0026quot;);  def e_score_dexp(nb_kmer, nb_read, genome_size): cov = nb_kmer/genome_size return((nb_kmer/nb_read) * double_exp(cov)) e_score_dexp_vec = np.vectorize(e_score_dexp)  Now let\u0026rsquo;s look at how this modified E-score performs\ne_score_dexp_a = e_score_dexp_vec(a_kmers, a_reads, genome_size) e_score_dexp_b = e_score_dexp_vec(b_kmers, b_reads, genome_size) e_score_dexp_c = e_score_dexp_vec(c_kmers, c_reads, genome_size) e_score_dexp_d = e_score_dexp_vec(d_kmers, d_reads, genome_size) plt.plot(e_score_dexp_a, label = \u0026quot;case_a\u0026quot;) plt.plot(e_score_dexp_b, label = \u0026quot;case_b\u0026quot;) plt.plot(e_score_dexp_c, label = \u0026quot;case_c\u0026quot;) plt.plot(e_score_dexp_d, label = \u0026quot;case_b\u0026quot;) plt.legend(loc=\u0026quot;upper left\u0026quot;) plt.title(\u0026quot;Double-exp E-score\u0026quot;) plt.ylim(0,4);  We can see that there is a much clearer distinction between true and false positive assignments. We can also look at the mean difference of all scenarios between true and false positives, for the original, and the new double exp E-score\ndiff_e_score_guellil = np.mean([e_score_guellil_b - e_score_guellil_a, e_score_guellil_c - e_score_guellil_a, e_score_guellil_d - e_score_guellil_a], axis=0) diff_e_score_dexp = np.mean([e_score_dexp_b - e_score_dexp_a, e_score_dexp_c - e_score_dexp_a, e_score_dexp_d - e_score_dexp_a], axis=0)  plt.plot(diff_e_score_guellil, label=\u0026quot;Guellil et al. E-score\u0026quot;) plt.plot(diff_e_score_dexp, label=\u0026quot;double-exp E-score\u0026quot;) plt.title(\u0026quot;Difference of E-score between true and false positive\\n(the greater the better)\u0026quot;) plt.legend(loc=\u0026quot;upper left\u0026quot;) plt.ylim(0,30);  df = pd.DataFrame(list(zip(diff_e_score_guellil, diff_e_score_dexp)), columns=['Guellil et al.', 'dexp'])  sns.boxplot(data=df) plt.title(\u0026quot;Difference of E-score between true and false positive\\n(the greater the better)\u0026quot;);  df.mean(axis=0)  Guellil et al. 1.012888 dexp 50.104112 dtype: float64  We can see that on average between all scenarios, for Guellil et al. E-score, there is only a difference of 1 point, while for the modified double-exponential E-score, there is an average difference of 50 points.\nConclusion In my simulations, using a the modified double-exponential E-score could allow for a better distinction between true and false positive taxonomic assignations by KrakenUniq.\nThe notebook used for this blog post is available here\n","date":1652182725,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652182725,"objectID":"ac0d5f85e58423f42c7c80c691ba65b7","permalink":"https://maximeborry.com/post/kraken-uniq/","publishdate":"2022-05-10T13:38:45+02:00","relpermalink":"/post/kraken-uniq/","section":"post","summary":"How to distinguish between true and false positive assignations with KrakenUniq","tags":["kraken","metagenomics","duplication"],"title":"A new E-score for KrakenUniq","type":"post"},{"authors":["Maxime Borry","Alexander H√ºbner","Adam B. Rohrlach","Christina Warinner"],"categories":[],"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627387108,"objectID":"916676db1204d1a8cb1b4c794865ac9f","permalink":"https://maximeborry.com/publication/borry-pydamage-2021/","publishdate":"2021-07-27T11:58:23.995025Z","relpermalink":"/publication/borry-pydamage-2021/","section":"publication","summary":"DNA de novo assembly can be used to reconstruct longer stretches of DNA (contigs), including genes and even genomes, from short DNA sequencing reads. Applying this technique to metagenomic data derived from archaeological remains, such as paleofeces and dental calculus, we can investigate past microbiome functional diversity that may be absent or underrepresented in the modern microbiome gene catalogue. However, compared to modern samples, ancient samples are often burdened with environmental contamination, resulting in metagenomic datasets that represent mixtures of ancient and modern DNA. The ability to rapidly and reliably establish the authenticity and integrity of ancient samples is essential for ancient DNA studies, and the ability to distinguish between ancient and modern sequences is particularly important for ancient microbiome studies. Characteristic patterns of ancient DNA damage, namely DNA fragmentation and cytosine deamination (observed as C-to-T transitions) are typically used to authenticate ancient samples and sequences, but existing tools for inspecting and filtering aDNA damage either compute it at the read level, which leads to high data loss and lower quality when used in combination with de novo assembly, or require manual inspection, which is impractical for ancient assemblies that typically contain tens to hundreds of thousands of contigs. To address these challenges, we designed PyDamage, a robust, automated approach for aDNA damage estimation and authentication of de novo assembled aDNA. PyDamage uses a likelihood ratio based approach to discriminate between truly ancient contigs and contigs originating from modern contamination. We test PyDamage on both on simulated aDNA data and archaeological paleofeces, and we demonstrate its ability to reliably and automatically identify contigs bearing DNA damage characteristic of aDNA. Coupled with aDNA de novo assembly, Pydamage opens up new doors to explore functional diversity in ancient metagenomic datasets.","tags":[],"title":"PyDamage: automated ancient damage identification and estimation for contigs in ancient DNA de novo assembly","type":"publication"},{"authors":["Marsha C. Wibowo","Zhen Yang","Maxime Borry","Alexander H√ºbner","Kun D. Huang","Braden T. Tierney","Samuel Zimmerman","Francisco Barajas-Olmos","Cecilia Contreras-Cubas","Humberto Garc√≠a-Ortiz","Ang√©lica Mart√≠nez-Hern√°ndez","Jacob M. Luber","Philipp Kirstahler","Tre Blohm","Francis E. Smiley","Richard Arnold","Sonia A. Ballal","S√ºnje Johanna Pamp","Julia Russ","Frank Maixner","Omar Rota-Stabelli","Nicola Segata","Karl Reinhard","Lorena Orozco","Christina Warinner","Meradeth Snow","Steven LeBlanc","Aleksandar D. Kostic"],"categories":[],"content":"","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621345981,"objectID":"eb0f8816971a16a188b7b2e903005af1","permalink":"https://maximeborry.com/publication/wibowo-reconstruction-2021/","publishdate":"2021-05-18T13:53:01.058822Z","relpermalink":"/publication/wibowo-reconstruction-2021/","section":"publication","summary":"Loss of gut microbial diversity in industrial populations is associated with chronic diseases, underscoring the importance of studying our ancestral gut microbiome. However, relatively little is known about the composition of pre-industrial gut microbiomes. Here we performed a large-scale de novo assembly of microbial genomes from palaeofaeces. From eight authenticated human palaeofaeces samples (1,000‚Äì2,000¬†years old) with well-preserved DNA from southwestern USA and Mexico, we reconstructed 498 medium- and high-quality microbial genomes. Among the 181 genomes with the strongest evidence of being ancient and of human gut origin, 39% represent previously undescribed species-level genome bins. Tip dating suggests an approximate diversification timeline for the key human symbiont Methanobrevibacter smithii. In comparison to 789 present-day human gut microbiome samples from eight countries, the palaeofaeces samples are more similar to non-industrialized than industrialized human gut microbiomes. Functional profiling of the palaeofaeces samples reveals a markedly lower abundance of antibiotic-resistance and mucin-degrading genes, as well as enrichment of mobile genetic elements relative to industrial gut microbiomes. This study facilitates the discovery and characterization of previously undescribed gut microorganisms from ancient microbiomes and the investigation of the evolutionary history of the human gut microbiota through genome reconstruction from palaeofaeces.","tags":[],"title":"Reconstruction of ancient microbial genomes from the human gut","type":"publication"},{"authors":["James A. Fellows Yates","Thiseas C. Lamnidis","Maxime Borry","Aida Andrades Valtue√±a","Zandra Fagern√§s","Stephen Clayton","Maxime U. Garcia","Judith Neukamm","Alexander Peltzer"],"categories":[],"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615894358,"objectID":"99a2d7782ef8d167786f1fe5f184dda1","permalink":"https://maximeborry.com/publication/yates-reproducible-2021/","publishdate":"2021-03-16T11:32:34.817809Z","relpermalink":"/publication/yates-reproducible-2021/","section":"publication","summary":"The broadening utilisation of ancient DNA to address archaeological, palaeontological, and biological questions is resulting in a rising diversity in the size of laboratories and scale of analyses being performed. In the context of this heterogeneous landscape, we present an advanced, and entirely redesigned and extended version of the EAGER pipeline for the analysis of ancient genomic data. This Nextflow pipeline aims to address three main themes: accessibility and adaptability to different computing configurations, reproducibility to ensure robust analytical standards, and updating the pipeline to the latest routine ancient genomic practices. The new version of EAGER has been developed within the nf-core initiative to ensure high-quality software development and maintenance support; contributing to a long-term life-cycle for the pipeline. nf-core/eager will assist in ensuring that a wider range of ancient DNA analyses can be applied by a diverse range of research groups and fields.","tags":[],"title":"Reproducible, portable, and efficient ancient genome reconstruction with nf-core/eager","type":"publication"},{"authors":[""],"categories":[],"content":"Recently, I\u0026rsquo;ve been working on a project where I need to store a key-value pairs for very large files. In Python, the first idea would be to use a dictionary.\nHowever, I had more than 800 millions key-value pairs to store, so a dictonary just wouldn\u0026rsquo;t cut it.\nEnters the world of noSQL databases, and more specifically RocksDB.\nDescribed by its creators, originally at Facebook, as a \u0026ldquo;high performance persistant key-value store\u0026rdquo;, it behaves pretty much like a dictonary, but can scale very well.\nWritten in C++, there is fortunately a python binding, in the name of python-rocksdb, even available as a conda package.\nIt\u0026rsquo;s relatively simple to use:\nimport rocksdb # Create or open database db = rocksdb.DB(\u0026quot;test.db\u0026quot;, rocksdb.Options(create_if_missing=True)) key = \u0026quot;Hello\u0026quot; value = \u0026quot;World\u0026quot; # Store key-value pair db.put(key.encode(), value.decode()) # Get value for a given key print(db.get(key.encode()).decode()) # prints \u0026quot;World\u0026quot; # Delete a key value-pair db.delete(key.encode())  Because RocksDB stores all data as byte strings, one need to use .encode() and .decode() to store and get data.\nBatch operation One way to speed up RockDB, is to perform operations in batch, instead of performing them one by one.\nimport rocksdb db = rocksdb.DB(\u0026quot;test.db\u0026quot;, rocksdb.Options(create_if_missing=True)) batch = rocksdb.WriteBatch() batch.put(b\u0026quot;key\u0026quot;, b\u0026quot;v1\u0026quot;) batch.delete(b\u0026quot;key\u0026quot;) batch.put(b\u0026quot;key\u0026quot;, b\u0026quot;v2\u0026quot;) batch.put(b\u0026quot;key\u0026quot;, b\u0026quot;v3\u0026quot;) db.write(batch)  Batch limits and solutions But beware ! For very large files, the batch can become too big for the memory of your computer, and you end up swapping like crazy !\n  When memory overflows to swap  To prevent this, we have to restrict ourselves to batches of reasonable sizes, meaning making more batches, but smaller.\nBut beware ! RocksDB storage system relies on many indivual .sst files, that RocksDB opens in parallel to make queries and store data. The larger the database, the more files are open. And this can lead to this kind of error:\nIO error: While open a file for appending: xxxxxxx.sst: Too many open files  Indeed, there is maximum number of files that can be open at any give time by a single process.\nFor example, it\u0026rsquo;s 256 by default on macOS\nLuckily, to overcome this hurdle, RocksDB has the max_open_files option.\nPutting it all together, this gives us the following script:\nimport rocksdb from subprocess import check_output from tqdm import tqdm def get_nb_lines(filename): \u0026quot;\u0026quot;\u0026quot;A function for getting the number of lines in a file Args: filename(str): The path to a file Returns: int: Number of lines in file \u0026quot;\u0026quot;\u0026quot; cmd = f\u0026quot;wc -l {filename}\u0026quot; return int(check_output(cmd, shell=True).split()[0]) OPTS = rocksdb.Options() OPTS.create_if_missing = False OPTS.max_open_files = 250 # Instantiating the database db = rocksdb.DB(\u0026quot;mybigdata.db\u0026quot;, OPTS) # The big file we want to store in RocksDB key_value_file = \u0026quot;very_large_file.tsv\u0026quot; # We get the number of key-value pairs in the file nb_key_value_pairs = get_nb_lines(key_value_file) # Starting our first batch batch = rocksdb.WriteBatch() # Key-value pair counter i = 0 # Number of batches, the more, the less memory used max_batches = 100 # Setting the batch size: how many key-value pairs go in each batch batch_size = min(nlines-1, int(nlines/max_batches)) with open(key_value_file) as bigfile: for line in tqdm(bigfile, total=nlines): # Each line looks like: key [tab] value linesplit = line.split() key = linesplit[0] value = linesplit[1] batch.put(bytes(key, encoding='utf8'), bytes(value, encoding='utf8')) # If we reached the batch size, store it in RocksDB, and create a new batch if i % batch_size == 0: db.write(batch) batch = rocksdb.WriteBatch() # Store the remaining key-value pairs db.write(batch)  ","date":1612950021,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612950801,"objectID":"d87433cfdaba0b27f66c45dad4b64c89","permalink":"https://maximeborry.com/post/rocksdb/","publishdate":"2021-02-10T10:40:21+01:00","relpermalink":"/post/rocksdb/","section":"post","summary":"Recently, I\u0026rsquo;ve been working on a project where I need to store a key-value pairs for very large files. In Python, the first idea would be to use a dictionary.","tags":["noSQL","database","big data"],"title":"noSQL: when it's too big for a dictionary","type":"post"},{"authors":[],"categories":[],"content":"Recently, I\u0026rsquo;ve been involved with the AncientMetagenomeDir project. Briefly, with this collaborative community effort, we aimed to regroup in one single repository, all the metadata about every single published ancient DNA metagenomics article, and turn them into FAIR scientific data.\nWe ended with large TSV (table) files regrouping a standardized set of metadata, about each ancient DNA metagenomics sample. Because these are originally archeological data, one of the information that is systematically collected is the geographical location of each sample.\nWhile static maps were already generated for the AncientMegenomeDir publication, we had the opportunity to play with interactive maps for the website of the project.\nUsually, hosting interactive elements online require some sort of backend framework (like Streamlit or Shiny) to perform the rendering, however, I wanted to have it as serverless as possible, and this is where the GeoJSON rendering function of GitHub came to the rescue.\nUsing GitHub magic, that meant that as long as I would push a GeoJSON file on GitHub, it would automatically be rendered as an interactive map, thanks to Leaflet.js.\nFrom TSV to GeoJSON The question that I was left with: How to go from a TSV table to a GeoJSON file ? Luckily for me, this is really easy to do thanks to GeoPandas.\nI only needed to make sure that there were a latitude and longitude columns in the TSV files.\n Geographic coordinate system, credit: Wikipedia  import pandas as pd import geopandas df = pd.read_csv(\u0026quot;table.tsv\u0026quot;, sep=\u0026quot;\\t\u0026quot;) gdf = geopandas.GeoDataFrame(df, geometry=geopandas.points_from_xy(df.longitude, df.latitude)) gdf.to_file(\u0026quot;output.geo.json\u0026quot;, driver='GeoJSON')   Instead of pushing to GitHub at every change to check a GeoJSON rendering, you can check a GeoJSON map with the geojson.io website.\n Displaying more metadata on the map So far, I only used the map to display the latitude and longitude of each sample, but we can actually display more information by changing the color, size, or the shape of each marker(point) for example.\nRefering again the Github documentation, this corresponds to the marker-color, marker-size, or marker-symbol.\nFor example, to change the color, we add a marker-color column with the desired color value.\n   marker-color publication_doi site_name latitude longitude sample_name sample_age material archive archive_accession     #009C54 10.1016/j.quascirev.2017.11.037 H√§sseldala Port 56.16 15.01 HA1.1 13900 lake sediment ENA SRS2040659   #C22026 10.3390/geosciences10070270 Unknown 53.322 1.118 ELF001A_95_S81_ELFM1D1 6000 shallow marine sediment ENA ERS3605424    Table 1: Sample data from the AncientMetagenomeDir repository\n Markers are colored by property  Here, markers in pink are host-associated single genomes, while markers in light-blue are host-associated metagenomes.\nPreventing overlapping points In this dataset, different samples are sometimes coming from a same archeological site. In practice, this means that points will overlap on the map because they share the exact same geographic coordinates. In Figure 2, for example, you can notice a very dark shadow bellow each marker: that\u0026rsquo;s because there are many overlapping markers present on the spot.\nThe problem is that only one marker will be displayed, and the other ones being hidden below.\nTo overcome this issue, the little trick is to slightly alter the coordinates of each sample to plot them as distinct points on the map. I did that with random sampling from the normal distribution using Numpy with a very small standard deviation.\nimport pandas as pd import geopandas import numpy as np df = pd.read_csv(\u0026quot;table.tsv\u0026quot;, sep=\u0026quot;\\t\u0026quot;) sigma = 0.0015 df['new_latitude'] = df['latitude'].apply(lambda x: np.random.normal(x, sigma)) df['new_longitude'] = df['longitude'].apply(lambda x: np.random.normal(x, sigma)) gdf = geopandas.GeoDataFrame(df, geometry=geopandas.points_from_xy(df.new_longitude, df.new_latitude)) gdf.to_file(\u0026quot;output.geo.json\u0026quot;, driver='GeoJSON')  Problem solved !\n Overalapping markers are \u0026lsquo;jittered\u0026rsquo; around the exact coordinates  End result Finally, thanks to the magic of GitHub GeoJSON rendering, the map can be easily embedded on any web page !\n This is an interactive map, you can click on a marker to look at the details.\n","date":1610453310,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610453310,"objectID":"10d879789c08f77fbf1dbdbebe351ede","permalink":"https://maximeborry.com/post/interactive-map-howto/","publishdate":"2021-01-12T13:08:30+01:00","relpermalink":"/post/interactive-map-howto/","section":"post","summary":"Recently, I\u0026rsquo;ve been involved with the AncientMetagenomeDir project. Briefly, with this collaborative community effort, we aimed to regroup in one single repository, all the metadata about every single published ancient DNA metagenomics article, and turn them into FAIR scientific data.","tags":[],"title":"Making Interactive maps in Python using GeoJSON and GitHub","type":"post"},{"authors":["James A. Fellows Yates","Aida Andrades Valtue√±a","√Öshild J. V√•gene","Becky Cribdon","Irina M. Velsko","Maxime Borry","Miriam J. Bravo-Lopez","Antonio Fernandez-Guerra","Eleanor J. Green","Shreya L. Ramachandran","Peter D. Heintzman","Maria A. Spyrou","Alexander H√ºbner","Abigail S. Gancz","Jessica Hider","Aurora F. Allshouse","Valentina Zaro","Christina Warinner"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611672223,"objectID":"00b11b4b61998b4bf340df09d859cef9","permalink":"https://maximeborry.com/publication/fellows-yates-community-curated-2021/","publishdate":"2021-01-26T14:43:37.845793Z","relpermalink":"/publication/fellows-yates-community-curated-2021/","section":"publication","summary":"Ancient DNA and RNA are valuable data sources for a wide range of disciplines. Within the field of ancient metagenomics, the number of published genetic datasets has risen dramatically in recent years, and tracking this data for reuse is particularly important for large-scale ecological and evolutionary studies of individual taxa and communities of both microbes and eukaryotes. AncientMetagenomeDir is a collection of annotated metagenomic sample lists derived from published studies that provide basic, standardised metadata and accession numbers to allow rapid data retrieval from online repositories. These tables are community-curated and span multiple sub-disciplines to ensure adequate breadth and consensus in metadata definitions, as well as longevity of the database. Internal guidelines and automated checks facilitate compatibility with established sequence-read archives and term-ontologies, and ensure consistency and interoperability for future meta-analyses. This collection will also assist in standardising metadata reporting for future ancient metagenomic studies.","tags":[],"title":"Community-curated and standardised metadata of published ancient metagenomic samples with AncientMetagenomeDir","type":"publication"},{"authors":["Maxime Borry et al"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"a6c841878089f7ea449b41187a425d5b","permalink":"https://maximeborry.com/publication/borry-coproid-2020/","publishdate":"2020-05-04T10:05:30.663951Z","relpermalink":"/publication/borry-coproid-2020/","section":"publication","summary":"Shotgun metagenomics applied to archaeological feces (paleofeces) can bring new insights into the composition and functions of human and animal gut microbiota from the past. However, paleofeces often undergo physical distortions in archaeological sediments, making their source species difficult to identify on the basis of fecal morphology or microscopic features alone. Here we present a reproducible and scalable pipeline using both host and microbial DNA to infer the host source of fecal material. We apply this pipeline to newly sequenced archaeological specimens and show that we are able to distinguish morphologically similar human and canine paleofeces, as well as non-fecal sediments, from a range of archaeological contexts.","tags":null,"title":"CoproID predicts the source of coprolites and paleofeces using microbiome composition and host DNA content","type":"publication"},{"authors":[""],"categories":[],"content":"","date":1578047467,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578047467,"objectID":"ef1f1688bdc9cf908c3f39b559c69174","permalink":"https://maximeborry.com/project/cogsea/","publishdate":"2020-01-03T11:31:07+01:00","relpermalink":"/project/cogsea/","section":"project","summary":"comparative Analysis of Gene Set Enrichment analysis. R and transcriptomics. Also available as a Shiny GUI.","tags":["Bioinformatics"],"title":"coGSEA","type":"project"},{"authors":[""],"categories":[],"content":"","date":1578047467,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578047467,"objectID":"94098fb1b327af96329288f347b36a8d","permalink":"https://maximeborry.com/project/dixit_diderot/","publishdate":"2020-01-03T11:31:07+01:00","relpermalink":"/project/dixit_diderot/","section":"project","summary":"Recreate the writing of Philosopher Denis Diderot using Markov Chains.","tags":["Software"],"title":"Dixit Diderot","type":"project"},{"authors":[""],"categories":[],"content":"","date":1578047467,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578047467,"objectID":"f3bbecb37b07681cc109ad371457cec1","permalink":"https://maximeborry.com/project/meetdock/","publishdate":"2020-01-03T11:31:07+01:00","relpermalink":"/project/meetdock/","section":"project","summary":"A scoring method for protein-protein docking.","tags":["Bioinformatics","Machine Learning"],"title":"Meetdock","type":"project"},{"authors":[""],"categories":[],"content":"","date":1578047467,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578047467,"objectID":"971e2a99f739f5333f54211e41bed7e2","permalink":"https://maximeborry.com/project/anacyclus/","publishdate":"2020-01-03T11:31:07+01:00","relpermalink":"/project/anacyclus/","section":"project","summary":"Phylogeny of this Moroccan medicinal plant Anacyclus genus using the internal transcribed spacer marker.","tags":["Data Analysis"],"title":"Phylogeny of the Anacyclus genus","type":"project"},{"authors":["Maxime Borry"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"67a4f95edc7c2b4f75e44605a3e4efe4","permalink":"https://maximeborry.com/publication/borry-sourcepredict-2019/","publishdate":"2020-05-04T10:05:54.370188Z","relpermalink":"/publication/borry-sourcepredict-2019/","section":"publication","summary":"SourcePredict is a Python package distributed through Conda, to classify and predict the origin of metagenomic samples, given a reference dataset of known origins, a problem also known as source tracking.","tags":null,"title":"Sourcepredict: Prediction of metagenomic sample sources using dimension reduction followed by machine learning classification","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://maximeborry.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]
[{"authors":["maxime"],"categories":null,"content":"I am Maxime, a PhD student in Bioinformatics at the Max Planck Institute for the Science History, in Jena, Germany\nAfter an undergraduate degree in biology, I first decided to dive in the studies of population and our environment. Only after realizing I wanted to focus more on the statistical part and the data analysis, I switched back to a Bioinformatics MSc.\nI am now working in the Microbiome Sciences research group on ancient DNA metagenomics, under the supervision of Christina Warinner.\n","date":1612950021,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1612950801,"objectID":"e039f055c184f6b656cae4531387a7fc","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am Maxime, a PhD student in Bioinformatics at the Max Planck Institute for the Science History, in Jena, Germany\nAfter an undergraduate degree in biology, I first decided to dive in the studies of population and our environment.","tags":null,"title":"","type":"authors"},{"authors":[""],"categories":[],"content":"Recently, I\u0026rsquo;ve been working on a project where I need to store a key-value pairs for very large files. In Python, the first idea would be to use a dictionary.\nHowever, I had more than 800 millions key-value pairs to store, so a dictonary just wouldn\u0026rsquo;t cut it.\nEnters the world of noSQL databases, and more specifically RocksDB.\nDescribed by its creators, originally at Facebook, as a \u0026ldquo;high performance persistant key-value store\u0026rdquo;, it behaves pretty much like a dictonary, but can scale very well.\nWritten in C++, there is fortunately a python binding, in the name of python-rocksdb, even available as a conda package.\nIt\u0026rsquo;s relatively simple to use:\nimport rocksdb # Create or open database db = rocksdb.DB(\u0026quot;test.db\u0026quot;, rocksdb.Options(create_if_missing=True)) key = \u0026quot;Hello\u0026quot; value = \u0026quot;World\u0026quot; # Store key-value pair db.put(key.encode(), value.decode()) # Get value for a given key print(db.get(key.encode()).decode()) # prints \u0026quot;World\u0026quot; # Delete a key value-pair db.delete(key.encode())  Because RocksDB stores all data as byte strings, one need to use .encode() and .decode() to store and get data.\nBatch operation One way to speed up RockDB, is to perform operations in batch, instead of performing them one by one.\nimport rocksdb db = rocksdb.DB(\u0026quot;test.db\u0026quot;, rocksdb.Options(create_if_missing=True)) batch = rocksdb.WriteBatch() batch.put(b\u0026quot;key\u0026quot;, b\u0026quot;v1\u0026quot;) batch.delete(b\u0026quot;key\u0026quot;) batch.put(b\u0026quot;key\u0026quot;, b\u0026quot;v2\u0026quot;) batch.put(b\u0026quot;key\u0026quot;, b\u0026quot;v3\u0026quot;) db.write(batch)  Batch limits and solutions But beware ! For very large files, the batch can become too big for the memory of your computer, and you end up swapping like crazy !\n  When memory overflows to swap   To prevent this, we have to restrict ourselves to batches of reasonable sizes, meaning making more batches, but smaller.\nBut beware ! RocksDB storage system relies on many indivual .sst files, that RocksDB opens in parallel to make queries and store data. The larger the database, the more files are open. And this can lead to this kind of error:\nIO error: While open a file for appending: xxxxxxx.sst: Too many open files  Indeed, there is maximum number of files that can be open at any give time by a single process.\nFor example, it\u0026rsquo;s 256 by default on macOS\nLuckily, to overcome this hurdle, RocksDB has the max_open_files option.\nPutting it all together, this gives us the following script:\nimport rocksdb from subprocess import check_output from tqdm import tqdm def get_nb_lines(filename): \u0026quot;\u0026quot;\u0026quot;A function for getting the number of lines in a file Args: filename(str): The path to a file Returns: int: Number of lines in file \u0026quot;\u0026quot;\u0026quot; cmd = f\u0026quot;wc -l {filename}\u0026quot; return int(check_output(cmd, shell=True).split()[0]) OPTS = rocksdb.Options() OPTS.create_if_missing = False OPTS.max_open_files = 250 # Instantiating the database db = rocksdb.DB(\u0026quot;mybigdata.db\u0026quot;, OPTS) # The big file we want to store in RocksDB key_value_file = \u0026quot;very_large_file.tsv\u0026quot; # We get the number of key-value pairs in the file nb_key_value_pairs = get_nb_lines(key_value_file) # Starting our first batch batch = rocksdb.WriteBatch() # Key-value pair counter i = 0 # Number of batches, the more, the less memory used max_batches = 100 # Setting the batch size: how many key-value pairs go in each batch batch_size = min(nlines-1, int(nlines/max_batches)) with open(key_value_file) as bigfile: for line in tqdm(bigfile, total=nlines): # Each line looks like: key [tab] value linesplit = line.split() key = linesplit[0] value = linesplit[1] batch.put(bytes(key, encoding='utf8'), bytes(value, encoding='utf8')) # If we reached the batch size, store it in RocksDB, and create a new batch if i % batch_size == 0: db.write(batch) batch = rocksdb.WriteBatch() # Store the remaining key-value pairs db.write(batch)  ","date":1612950021,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612950801,"objectID":"d87433cfdaba0b27f66c45dad4b64c89","permalink":"https://maximeborry.com/post/rocksdb/","publishdate":"2021-02-10T10:40:21+01:00","relpermalink":"/post/rocksdb/","section":"post","summary":"Recently, I\u0026rsquo;ve been working on a project where I need to store a key-value pairs for very large files. In Python, the first idea would be to use a dictionary.","tags":["noSQL","database","big data"],"title":"noSQL: when it's too big for a dictionary","type":"post"},{"authors":[],"categories":[],"content":"Recently, I\u0026rsquo;ve been involved with the AncientMetagenomeDir project. Briefly, with this collaborative community effort, we aimed to regroup in one single repository, all the metadata about every single published ancient DNA metagenomics article, and turn them into FAIR scientific data.\nWe ended with large TSV (table) files regrouping a standardized set of metadata, about each ancient DNA metagenomics sample. Because these are originally archeological data, one of the information that is systematically collected is the geographical location of each sample.\nWhile static maps were already generated for the AncientMegenomeDir publication, we had the opportunity to play with interactive maps for the website of the project.\nUsually, hosting interactive elements online require some sort of backend framework (like Streamlit or Shiny) to perform the rendering, however, I wanted to have it as serverless as possible, and this is where the GeoJSON rendering function of GitHub came to the rescue.\nUsing GitHub magic, that meant that as long as I would push a GeoJSON file on GitHub, it would automatically be rendered as an interactive map, thanks to Leaflet.js.\nFrom TSV to GeoJSON The question that I was left with: How to go from a TSV table to a GeoJSON file ? Luckily for me, this is really easy to do thanks to GeoPandas.\nI only needed to make sure that there were a latitude and longitude columns in the TSV files.\n Geographic coordinate system, credit: Wikipedia   import pandas as pd import geopandas df = pd.read_csv(\u0026quot;table.tsv\u0026quot;, sep=\u0026quot;\\t\u0026quot;) gdf = geopandas.GeoDataFrame(df, geometry=geopandas.points_from_xy(df.longitude, df.latitude)) gdf.to_file(\u0026quot;output.geo.json\u0026quot;, driver='GeoJSON')   Instead of pushing to GitHub at every change to check a GeoJSON rendering, you can check a GeoJSON map with the geojson.io website.\n Displaying more metadata on the map So far, I only used the map to display the latitude and longitude of each sample, but we can actually display more information by changing the color, size, or the shape of each marker(point) for example.\nRefering again the Github documentation, this corresponds to the marker-color, marker-size, or marker-symbol.\nFor example, to change the color, we add a marker-color column with the desired color value.\n   marker-color publication_doi site_name latitude longitude sample_name sample_age material archive archive_accession     #009C54 10.1016/j.quascirev.2017.11.037 HÃ¤sseldala Port 56.16 15.01 HA1.1 13900 lake sediment ENA SRS2040659   #C22026 10.3390/geosciences10070270 Unknown 53.322 1.118 ELF001A_95_S81_ELFM1D1 6000 shallow marine sediment ENA ERS3605424    Table 1: Sample data from the AncientMetagenomeDir repository\n Markers are colored by property   Here, markers in pink are host-associated single genomes, while markers in light-blue are host-associated metagenomes.\nPreventing overlapping points In this dataset, different samples are sometimes coming from a same archeological site. In practice, this means that points will overlap on the map because they share the exact same geographic coordinates. In Figure 2, for example, you can notice a very dark shadow bellow each marker: that\u0026rsquo;s because there are many overlapping markers present on the spot.\nThe problem is that only one marker will be displayed, and the other ones being hidden below.\nTo overcome this issue, the little trick is to slightly alter the coordinates of each sample to plot them as distinct points on the map. I did that with random sampling from the normal distribution using Numpy with a very small standard deviation.\nimport pandas as pd import geopandas import numpy as np df = pd.read_csv(\u0026quot;table.tsv\u0026quot;, sep=\u0026quot;\\t\u0026quot;) sigma = 0.0015 df['new_latitude'] = df['latitude'].apply(lambda x: np.random.normal(x, sigma)) df['new_longitude'] = df['longitude'].apply(lambda x: np.random.normal(x, sigma)) gdf = geopandas.GeoDataFrame(df, geometry=geopandas.points_from_xy(df.new_longitude, df.new_latitude)) gdf.to_file(\u0026quot;output.geo.json\u0026quot;, driver='GeoJSON')  Problem solved !\n Overalapping markers are \u0026lsquo;jittered\u0026rsquo; around the exact coordinates   End result Finally, thanks to the magic of GitHub GeoJSON rendering, the map can be easily embedded on any web page !\n This is an interactive map, you can click on a marker to look at the details.\n","date":1610453310,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610453310,"objectID":"10d879789c08f77fbf1dbdbebe351ede","permalink":"https://maximeborry.com/post/interactive-map-howto/","publishdate":"2021-01-12T13:08:30+01:00","relpermalink":"/post/interactive-map-howto/","section":"post","summary":"Recently, I\u0026rsquo;ve been involved with the AncientMetagenomeDir project. Briefly, with this collaborative community effort, we aimed to regroup in one single repository, all the metadata about every single published ancient DNA metagenomics article, and turn them into FAIR scientific data.","tags":[],"title":"Making Interactive maps in Python using GeoJSON and GitHub","type":"post"},{"authors":["James A. Fellows Yates","Aida Andrades ValtueÃ±a","Ãshild J. VÃ¥gene","Becky Cribdon","Irina M. Velsko","Maxime Borry","Miriam J. Bravo-Lopez","Antonio Fernandez-Guerra","Eleanor J. Green","Shreya L. Ramachandran","Peter D. Heintzman","Maria A. Spyrou","Alexander HÃ¼bner","Abigail S. Gancz","Jessica Hider","Aurora F. Allshouse","Valentina Zaro","Christina Warinner"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611672223,"objectID":"00b11b4b61998b4bf340df09d859cef9","permalink":"https://maximeborry.com/publication/fellows-yates-community-curated-2021/","publishdate":"2021-01-26T14:43:37.845793Z","relpermalink":"/publication/fellows-yates-community-curated-2021/","section":"publication","summary":"Ancient DNA and RNA are valuable data sources for a wide range of disciplines. Within the field of ancient metagenomics, the number of published genetic datasets has risen dramatically in recent years, and tracking this data for reuse is particularly important for large-scale ecological and evolutionary studies of individual taxa and communities of both microbes and eukaryotes. AncientMetagenomeDir is a collection of annotated metagenomic sample lists derived from published studies that provide basic, standardised metadata and accession numbers to allow rapid data retrieval from online repositories. These tables are community-curated and span multiple sub-disciplines to ensure adequate breadth and consensus in metadata definitions, as well as longevity of the database. Internal guidelines and automated checks facilitate compatibility with established sequence-read archives and term-ontologies, and ensure consistency and interoperability for future meta-analyses. This collection will also assist in standardising metadata reporting for future ancient metagenomic studies.","tags":[],"title":"Community-curated and standardised metadata of published ancient metagenomic samples with AncientMetagenomeDir","type":"publication"},{"authors":["James A. Fellows Yates","Thiseas C. Lamnidis","Maxime Borry","Aida Andrades ValtueÃ±a","Zandra FagernÃ¤s","Stephen Clayton","Maxime U. Garcia","Judith Neukamm","Alexander Peltzer"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"91727c74c9980d38f0d45b51cebb101f","permalink":"https://maximeborry.com/publication/yates-reproducible-2020/","publishdate":"2020-06-15T10:10:49.439235Z","relpermalink":"/publication/yates-reproducible-2020/","section":"publication","summary":"The broadening utilisation of ancient DNA (aDNA) to address archaeological, palaeontological, and biological questions is resulting in a rising diversity in the size of laboratories and scale of analyses being performed. In the context of this heterogeneous landscape, we present nf-core/eager, an advanced and entirely redesigned pipeline for the analysis of ancient genomic data. nf-core/eager builds on existing ideas and concepts introduced in the original EAGER pipeline, and improves various aspects of the analysis procedure by employing computational frameworks such as Nextfow and nf-core. The pipeline aims to address three main themes: accessibility and adaptability to different research groups and their computing configurations, reproducibility to ensure robust analytical standards in the field, and updating the EAGER pipeline to the latest routine ancient genomic practises. This new version of EAGER has been developed within the nf-core initiative, to ensure high quality software development and maintenance support; contributing to a long-term lifecycle for the pipeline. nf-core/eager will assist in ensuring that ancient DNA sequencing data can be used by a diverse range of research groups and fields.textless/ptextgreater","tags":null,"title":"Reproducible, portable, and efficient ancient genome reconstruction with nf-core/eager","type":"publication"},{"authors":["Maxime Borry et al"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"a6c841878089f7ea449b41187a425d5b","permalink":"https://maximeborry.com/publication/borry-coproid-2020/","publishdate":"2020-05-04T10:05:30.663951Z","relpermalink":"/publication/borry-coproid-2020/","section":"publication","summary":"Shotgun metagenomics applied to archaeological feces (paleofeces) can bring new insights into the composition and functions of human and animal gut microbiota from the past. However, paleofeces often undergo physical distortions in archaeological sediments, making their source species difficult to identify on the basis of fecal morphology or microscopic features alone. Here we present a reproducible and scalable pipeline using both host and microbial DNA to infer the host source of fecal material. We apply this pipeline to newly sequenced archaeological specimens and show that we are able to distinguish morphologically similar human and canine paleofeces, as well as non-fecal sediments, from a range of archaeological contexts.","tags":null,"title":"CoproID predicts the source of coprolites and paleofeces using microbiome composition and host DNA content","type":"publication"},{"authors":[""],"categories":[],"content":"","date":1578047467,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578047467,"objectID":"ef1f1688bdc9cf908c3f39b559c69174","permalink":"https://maximeborry.com/project/cogsea/","publishdate":"2020-01-03T11:31:07+01:00","relpermalink":"/project/cogsea/","section":"project","summary":"comparative Analysis of Gene Set Enrichment analysis. R and transcriptomics. Also available as a Shiny GUI.","tags":["Bioinformatics"],"title":"coGSEA","type":"project"},{"authors":[""],"categories":[],"content":"","date":1578047467,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578047467,"objectID":"94098fb1b327af96329288f347b36a8d","permalink":"https://maximeborry.com/project/dixit_diderot/","publishdate":"2020-01-03T11:31:07+01:00","relpermalink":"/project/dixit_diderot/","section":"project","summary":"Recreate the writing of Philosopher Denis Diderot using Markov Chains.","tags":["Software"],"title":"Dixit Diderot","type":"project"},{"authors":[""],"categories":[],"content":"","date":1578047467,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578047467,"objectID":"f3bbecb37b07681cc109ad371457cec1","permalink":"https://maximeborry.com/project/meetdock/","publishdate":"2020-01-03T11:31:07+01:00","relpermalink":"/project/meetdock/","section":"project","summary":"A scoring method for protein-protein docking.","tags":["Bioinformatics","Machine Learning"],"title":"Meetdock","type":"project"},{"authors":[""],"categories":[],"content":"","date":1578047467,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578047467,"objectID":"971e2a99f739f5333f54211e41bed7e2","permalink":"https://maximeborry.com/project/anacyclus/","publishdate":"2020-01-03T11:31:07+01:00","relpermalink":"/project/anacyclus/","section":"project","summary":"Phylogeny of this Moroccan medicinal plant Anacyclus genus using the internal transcribed spacer marker.","tags":["Data Analysis"],"title":"Phylogeny of the Anacyclus genus","type":"project"},{"authors":["Maxime Borry"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"67a4f95edc7c2b4f75e44605a3e4efe4","permalink":"https://maximeborry.com/publication/borry-sourcepredict-2019/","publishdate":"2020-05-04T10:05:54.370188Z","relpermalink":"/publication/borry-sourcepredict-2019/","section":"publication","summary":"SourcePredict is a Python package distributed through Conda, to classify and predict the origin of metagenomic samples, given a reference dataset of known origins, a problem also known as source tracking.","tags":null,"title":"Sourcepredict: Prediction of metagenomic sample sources using dimension reduction followed by machine learning classification","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://maximeborry.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]